{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c8a7cc-de53-4725-8ae6-8c128e581b8b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "    \n",
    "### <center> GITHUB ISSUES</center>\n",
    "### <center> ELASTICSEARCH - OPEN AI</center>\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "<br>\n",
    "    <br>\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ffddc3a-6993-4848-bea0-6e392ae30da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.54.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (2.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\soham\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\soham\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.2)\n",
      "Requirement already satisfied: tzdata in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2024.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\soham\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c32c3-fdb5-4391-800f-38864eaaae01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8da7f1d-cc25-452e-9aa1-f019edfd90c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (8.15.1)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from elasticsearch) (8.15.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\soham\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "#Install elastic search\n",
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79588c-116b-4bc7-86df-e6ce16ce6f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7aee99d-0219-4469-9f8a-d1c32463ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import requests\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d15e9-bc44-4510-b3c5-40632ebc8299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b0610-c0c9-4e04-9068-d332231b904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the headers\n",
    "headers = {\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"access_token\": \"\",\n",
    "    \"Git_Username\":\"Enter your username\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f3b1c-0863-40a0-b3e0-40e7b26ce0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "efce5822-6d66-49b7-9523-ba0c3a074da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the owner and the repository\n",
    "owners = ['langchain-ai', 'microsoft', 'openai', 'elastic', 'milvus-io']\n",
    "repos = ['langchain', 'langgraph', 'autogen', 'openai-cookbook', 'elasticsearch', 'pymilvus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f4a9c-94f6-43d5-8d64-e5521cea24d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "decd06a3-5c46-4048-a22b-180bde820141",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "per_page = 100\n",
    "from_date = (dt.date.today() - dt.timedelta(days=60)).isoformat() #The duration for which we need the issues can be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116343df-7e18-4c8b-b285-f4bf00534000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0c0bc88-58d5-43ff-ad3c-9a8d3cb71e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that returns the base url\n",
    "def fetch_url(owner, repo):\n",
    "    return f\"https://\"+headers[\"Git_Username\"]+\":\"+headers[\"access_token\"]+f\"@api.github.com/repos/{owner}/{repo}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ace28-fedd-461a-92f8-76b870e673af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "515dc9c3-429f-4172-a3a9-0d9ba5e96752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://SFA24SCM66S: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@api.github.com/repos/langchain-ai/langchain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://SFA24SCM66S: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@api.github.com/repos/langchain-ai/langgraph\n",
      "https://SFA24SCM66S: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@api.github.com/repos/microsoft/autogen\n",
      "https://SFA24SCM66S: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@api.github.com/repos/openai/openai-cookbook\n",
      "https://SFA24SCM66S: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@api.github.com/repos/elastic/elasticsearch\n",
      "https://SFA24SCM66S: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@api.github.com/repos/milvus-io/pymilvus\n"
     ]
    }
   ],
   "source": [
    "# Fetching the Issues from the GitHub repository\n",
    "issues=[]\n",
    "for owner in owners:\n",
    "    for repo in repos:\n",
    "        if (owner=='langchain-ai' and repo=='langchain') or (owner=='langchain-ai' and repo=='langgraph') or (owner=='microsoft' and repo=='autogen') or (owner=='openai' and repo=='openai-cookbook') or (owner=='elastic' and repo=='elasticsearch') or (owner=='milvus-io' and repo=='pymilvus'):\n",
    "            flag = True\n",
    "            url = fetch_url(owner, repo)\n",
    "            print(url)\n",
    "            while flag:\n",
    "                    response = requests.get(f\"{url}/issues\", headers=headers,params={\"since\": from_date, \"page\": page,\"state\":\"all\"})\n",
    "                    for obj in response.json():\n",
    "                        if datetime.strptime(from_date, \"%Y-%m-%d\") <= datetime.strptime(obj[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\"):\n",
    "                            issueObject = {\n",
    "                            \"_type\": \"issue\",\n",
    "                            \"_repo\":repo,    \n",
    "                            \"_issueNumber\": str(obj['number']),\n",
    "                            \"_title\": str(obj['title']),\n",
    "                            \"_createdAt\": str(obj['created_at']),\n",
    "                            \"_closedAt\": str(obj['closed_at']) if str(obj['closed_at']) != \"None\" else \"2024-12-31T00:36:30Z\", # Few Issues might still be open, we add \"2024-12-31T00:36:30Z\" as closed date for those Issues.\n",
    "                            \"_state\": str(obj['state']),\n",
    "                            \"_body\": str(obj['body'])[:5000] \n",
    "                                # Here we are considering only the first 5000 characters from the body as \n",
    "                                # there is a limit on the the text tokens that we can embed using the openai model.\n",
    "                # Please refer https://platform.openai.com/docs/guides/embeddings to know more about the embedding models. \n",
    "        \n",
    "                # Please refer to https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb to see how tokens are counted.\n",
    "                        \n",
    "                            }\n",
    "                            issues.append(issueObject)                      \n",
    "                        else:\n",
    "                            flag = False\n",
    "                            break\n",
    "            \n",
    "                    if not response.ok or len(response.json()) == 0:\n",
    "                        break\n",
    "            \n",
    "                    page+=1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321aca6-0c87-43f7-b887-60b85666bd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7843a0f6-914f-417f-9733-179d3787498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_body': '### Checked other resources\\n'\n",
      "          '\\n'\n",
      "          '- [X] I added a very descriptive title to this issue.\\n'\n",
      "          '- [X] I searched the LangChain documentation with the integrated '\n",
      "          'search.\\n'\n",
      "          '- [X] I used the GitHub search to find a similar question and '\n",
      "          \"didn't find it.\\n\"\n",
      "          '- [X] I am sure that this is a bug in LangChain rather than my '\n",
      "          'code.\\n'\n",
      "          '- [X] The bug is not resolved by updating to the latest stable '\n",
      "          'version of LangChain (or the specific integration package).\\n'\n",
      "          '\\n'\n",
      "          '### Example Code\\n'\n",
      "          '\\n'\n",
      "          '```\\r\\n'\n",
      "          '#----------------\\r\\n'\n",
      "          '# HuggingFace embedding  (no issue)\\r\\n'\n",
      "          'from langchain_huggingface import HuggingFaceEmbeddings\\r\\n'\n",
      "          'embeddings = '\n",
      "          'HuggingFaceEmbeddings(model=\"sentence-transformers/all-mpnet-base-v2\")\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '#----------------\\r\\n'\n",
      "          '# create langchain-chroma persistent client with collection name '\n",
      "          \"'example_collection;  (no issue)\\r\\n\"\n",
      "          'from langchain_chroma import Chroma\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'vector_store = Chroma(\\r\\n'\n",
      "          '    collection_name=\"example_collection\",   # collection is \"table\" '\n",
      "          'in vectore store \\r\\n'\n",
      "          '    embedding_function=hf,    # hf is huggingface embeddings '\n",
      "          'derived  from the previous step \\r\\n'\n",
      "          '    persist_directory=\"./vectorstore/chroma_langchain_db\",  # Where '\n",
      "          'to save data locally, remove if not necessary\\r\\n'\n",
      "          ')\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '#----------------\\r\\n'\n",
      "          '# add at least one document into  vector collection (no issue)\\r\\n'\n",
      "          'from uuid import uuid4\\r\\n'\n",
      "          'from langchain_core.documents import Document\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'document_1 = Document(\\r\\n'\n",
      "          '    page_content=\"I had chocolate chip pancakes and scrambled eggs '\n",
      "          'for breakfast this morning.\",\\r\\n'\n",
      "          '    metadata={\"source\": \"tweet\"},\\r\\n'\n",
      "          '    id=1,\\r\\n'\n",
      "          ')\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'documents = [\\r\\n'\n",
      "          '    document_1,\\r\\n'\n",
      "          ']\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'uuids = [str(uuid4()) for _ in range(len(documents))]\\r\\n'\n",
      "          'vector_store.add_documents(documents=documents, ids=uuids)\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '#----------------  ERROR ENCOUNTERED when running get_by_ids \\r\\n'\n",
      "          '# attempt to run get_by_Ids yields NotImplementedError\\r\\n'\n",
      "          \"vector_store.get_by_ids(['6314982d-455f-47cc-bf97-6e5324f6af62'])\\r\\n\"\n",
      "          '\\r\\n'\n",
      "          '```\\n'\n",
      "          '\\n'\n",
      "          '### Error Message and Stack Trace (if applicable)\\n'\n",
      "          '\\n'\n",
      "          '{\\r\\n'\n",
      "          '\\t\"name\": \"NotImplementedError\",\\r\\n'\n",
      "          '\\t\"message\": \"Chroma does not yet support get_by_ids.\",\\r\\n'\n",
      "          '\\t\"stack\": '\n",
      "          '\"---------------------------------------------------------------------------\\r\\n'\n",
      "          'NotImplementedError                       Traceback (most recent '\n",
      "          'call last)\\r\\n'\n",
      "          'Cell In[87], line 3\\r\\n'\n",
      "          '      1 # testing get the first two document ids\\r\\n'\n",
      "          \"      2 # ids = ['db1e5f74-f18d-4765-a193-d30eaed7552f', \"\n",
      "          \"'12861b34-df54-4e40-8e1e-ae9ea901d378']\\r\\n\"\n",
      "          '----> 3 '\n",
      "          \"vector_store.get_by_ids(['6314982d-455f-47cc-bf97-6e5324f6af62'])\\r\\n\"\n",
      "          '      5 # get_by_ids() functionality is not avaiable until '\n",
      "          'v0.2.11\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'File '\n",
      "          '~/Documents/0_-_Python_Projects/05_Gen_AI/venv_3_11/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:164, '\n",
      "          'in VectorStore.get_by_ids(self, ids)\\r\\n'\n",
      "          '    140 \\\\\"\\\\\"\\\\\"Get documents by their IDs.\\r\\n'\n",
      "          '    141 \\r\\n'\n",
      "          '    142 The returned documents are expected to have the ID field '\n",
      "          'set to the ID of the\\r\\n'\n",
      "          '   (...)\\r\\n'\n",
      "          '    161 .. versionadded:: 0.2.11\\r\\n'\n",
      "          '    162 \\\\\"\\\\\"\\\\\"\\r\\n'\n",
      "          '    163 msg = f\\\\\"{self.__class__.__name__} does not yet support '\n",
      "          'get_by_ids.\\\\\"\\r\\n'\n",
      "          '--> 164 raise NotImplementedError(msg)\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'NotImplementedError: Chroma does not yet support get_by_ids.\"\\r\\n'\n",
      "          '}\\n'\n",
      "          '\\n'\n",
      "          '### Description\\n'\n",
      "          '\\n'\n",
      "          'I am just trying to run the vector_store method `get_by_ids`  - it '\n",
      "          'is listed as one of the available methods in '\n",
      "          '[here](https://python.langchain.com/api_reference/chroma/vectorstores/langchain_chroma.vectorstores.Chroma.html)\\r\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '### System Info\\n'\n",
      "          '\\n'\n",
      "          '$ python -m langchain_core.sys_info\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'System Information\\r\\n'\n",
      "          '------------------\\r\\n'\n",
      "          '> OS:  Darwin\\r\\n'\n",
      "          '> OS Version:  Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:00 '\n",
      "          'PDT 2024; root:xnu-10063.141.2~1/RELEASE_X86_64\\r\\n'\n",
      "          '> Python Version:  3.11.10 (main, Nov 19 2024, 15:24:32) [Clang '\n",
      "          '12.0.0 (clang-1200.0.32.29)]\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'Package Information\\r\\n'\n",
      "          '-------------------\\r\\n'\n",
      "          '> langchain_core: 0.3.19\\r\\n'\n",
      "          '> langchain: 0.3.7\\r\\n'\n",
      "          '> langchain_community: 0.3.4\\r\\n'\n",
      "          '> langsmith: 0.1.143\\r\\n'\n",
      "          '> langchain_chroma: 0.1.4\\r\\n'\n",
      "          '> langchain_experimental: 0.3.3\\r\\n'\n",
      "          '> langchain_groq: 0.2.1\\r\\n'\n",
      "          '> langchain_huggingface: 0.1.2\\r\\n'\n",
      "          '> langchain_text_splitters: 0.3.2\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'Optional packages not installed\\r\\n'\n",
      "          '-------------------------------\\r\\n'\n",
      "          '> langgraph\\r\\n'\n",
      "          '> langserve\\r\\n'\n",
      "          '\\r\\n'\n",
      "          'Other Dependencies\\r\\n'\n",
      "          '------------------\\r\\n'\n",
      "          '> aiohttp: 3.11.6\\r\\n'\n",
      "          '> async-timeout: Installed. No version info available.\\r\\n'\n",
      "          '> chromadb: 0.5.20\\r\\n'\n",
      "          '> dataclasses-json: 0.6.7\\r\\n'\n",
      "          '> fastapi: 0.115.5\\r\\n'\n",
      "          '> groq: 0.12.0\\r\\n'\n",
      "          '> httpx: 0.27.2\\r\\n'\n",
      "          '> httpx-sse: 0.4.0\\r\\n'\n",
      "          '> huggingface-hub: 0.26.2\\r\\n'\n",
      "          '> jsonpatch: 1.33\\r\\n'\n",
      "          '> numpy: 1.26.4\\r\\n'\n",
      "          '> orjson: 3.10.11\\r\\n'\n",
      "          '> packaging: 24.2\\r\\n'\n",
      "          '> pydantic: 2.9.2\\r\\n'\n",
      "          '> pydantic-settings: 2.6.1\\r\\n'\n",
      "          '> PyYAML: 6.0.2\\r\\n'\n",
      "          '> requests: 2.32.3\\r\\n'\n",
      "          '> requests-toolbelt: 1.0.0\\r\\n'\n",
      "          '> sentence-transformers: 3.3.1\\r\\n'\n",
      "          '> SQLAlchemy: 2.0.36\\r\\n'\n",
      "          '> tenacity: 9.0.0\\r\\n'\n",
      "          '> tokenizers: 0.20.3\\r\\n'\n",
      "          '> transformers: 4.46.3\\r\\n'\n",
      "          '> typing-extensions: 4.12.2',\n",
      " '_closedAt': '2024-12-31T00:36:30Z',\n",
      " '_createdAt': '2024-11-22T01:13:50Z',\n",
      " '_issueNumber': '28276',\n",
      " '_repo': 'langchain',\n",
      " '_state': 'open',\n",
      " '_title': 'langchain-chroma== 0.1.4   method get_by_ids is listed in '\n",
      "           'documentation BUT I am getting NotImplementedError',\n",
      " '_type': 'issue'}\n"
     ]
    }
   ],
   "source": [
    "#Sample Issue\n",
    "pprint(issues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449551ce-f1fd-4c1c-83d3-e4d24fa6ba49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1e6c3e7d-032d-414c-88af-db7d82cd4be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3911\n"
     ]
    }
   ],
   "source": [
    "#Number of Issues in the given timeframe\n",
    "pprint(len(issues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c232e5-aa18-4187-b838-3a3b1732ff35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "68599a46-1c6b-4897-a4e6-190059999f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of Issues to a DataFrame\n",
    "df_Issues = pd.DataFrame(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ecc4d-cf70-410f-bf16-68bb298a39ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a0a26fa-5fce-4e10-b2b1-5011a43d733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all NaN values with None in columns as elasticsearch does not recognize it\n",
    "df_Issues.fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76669868-930c-4af6-a8c9-c43b3f2f7af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0ce3ebd-bbd5-4edc-a5ab-ca1ef4c6779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embeddings from OpenAI API\n",
    "def embed(texts):\n",
    "    # Make a request to OpenAI API to get embeddings\n",
    "    embeddings = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model='text-embedding-ada-002'\n",
    "    )\n",
    "    # Extract embeddings from the API response\n",
    "    return [result.embedding for result in embeddings.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83e0c9-0b7b-48fa-a1a2-6eea57961218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df7bc0-1fd5-485b-a9c2-29283dfe7907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3911 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 500/3911 [01:04<07:19,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1000/3911 [02:07<06:10,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1500/3911 [03:11<05:07,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 2000/3911 [04:13<04:00,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 2500/3911 [05:16<02:58,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 3000/3911 [06:19<01:54,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3911/3911 [07:22<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411\n"
     ]
    }
   ],
   "source": [
    "## Embedding creation using openAI of GitHub Issues.\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client with API key\n",
    "client = OpenAI(api_key=\"Add your openai key here\")\n",
    "\n",
    "Issue_embeddings = []\n",
    "\n",
    "# Batch size for processing data\n",
    "batch_size = 500\n",
    "\n",
    "# Initialize data structure for storing text\n",
    "data = [\n",
    "    [], # Titles\n",
    "]\n",
    "count=0;\n",
    "# Embed and insert in batches\n",
    "for i in tqdm(range(0, len(df_Issues))):\n",
    "    title = str(df_Issues.iloc[i]['_title']).replace(\"\\n\", \"\") or ''\n",
    "    body = str(df_Issues.iloc[i]['_body']).replace(\"\\n\", \"\") or ''\n",
    "    \n",
    "    # Merge 'repository name','title' and 'body' of the GitHub Issue\n",
    "    combined_text = f\"Repository:{owner}/{repo} Issue Title:{title} Issue Body:{body}\"  \n",
    "    data[0].append(combined_text)\n",
    "    if len(data[0]) % batch_size == 0:\n",
    "        print(\"Embedding batch...\")\n",
    "\n",
    "        embeddings_batch = embed(data[0]) \n",
    "        Issue_embeddings.extend(embeddings_batch)\n",
    "        data = [[]]\n",
    "        print(\"Waiting for 1 minute before the next batch...\")\n",
    "        \n",
    "        time.sleep(60)    \n",
    "        \n",
    "# Embed the remaining data if any\n",
    "if len(data[0]) != 0:\n",
    "    embeddings_rem = embed(data[0])\n",
    "    print(len(embeddings_rem))\n",
    "    Issue_embeddings.extend(embeddings_rem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dacbf-ef0a-4f7c-b13f-427c37113eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a630d2f2-346a-4927-8bd6-761b097f09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Generated embeddings to GitHub_Issue_vector column in the dataframe\n",
    "\n",
    "df_Issues[\"GitHub_Issue_vector\"] = Issue_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca323fed-13c1-4636-bebb-dd7c3257e5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a3c634e-a209-4f00-9ed5-59bc16ad45df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_type</th>\n",
       "      <th>_repo</th>\n",
       "      <th>_issueNumber</th>\n",
       "      <th>_title</th>\n",
       "      <th>_createdAt</th>\n",
       "      <th>_closedAt</th>\n",
       "      <th>_state</th>\n",
       "      <th>_body</th>\n",
       "      <th>GitHub_Issue_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3906</th>\n",
       "      <td>issue</td>\n",
       "      <td>elasticsearch</td>\n",
       "      <td>113347</td>\n",
       "      <td>Update JDK version in CONTRIBUTING.md</td>\n",
       "      <td>2024-09-22T16:38:04Z</td>\n",
       "      <td>2024-09-26T21:20:39Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>This week, I set up a local development enviro...</td>\n",
       "      <td>[0.006106697954237461, 0.0008585970499552786, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3907</th>\n",
       "      <td>issue</td>\n",
       "      <td>elasticsearch</td>\n",
       "      <td>113346</td>\n",
       "      <td>deps(updatecli): bump all policies</td>\n",
       "      <td>2024-09-22T06:22:37Z</td>\n",
       "      <td>2024-09-23T06:22:34Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>\\n\\n\\n&lt;Actions&gt;\\n    &lt;action id=\"90caa11f1dfdd...</td>\n",
       "      <td>[-0.011111822910606861, -0.005381433758884668,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3908</th>\n",
       "      <td>issue</td>\n",
       "      <td>elasticsearch</td>\n",
       "      <td>113345</td>\n",
       "      <td>[CI] KibanaUserRoleIntegTests testSearchAndMSe...</td>\n",
       "      <td>2024-09-22T05:36:19Z</td>\n",
       "      <td>2024-09-25T10:16:00Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>**Build Scans:**\\n- [elasticsearch-periodic #4...</td>\n",
       "      <td>[0.004877804312855005, -0.006739743519574404, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>issue</td>\n",
       "      <td>elasticsearch</td>\n",
       "      <td>113344</td>\n",
       "      <td>[CI] RollupIndexerStateTests testMultipleJobTr...</td>\n",
       "      <td>2024-09-22T05:16:59Z</td>\n",
       "      <td>2024-11-13T17:28:03Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>**Build Scans:**\\n- [elasticsearch-periodic #4...</td>\n",
       "      <td>[-0.027301553636789322, -0.016045166179537773,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>issue</td>\n",
       "      <td>elasticsearch</td>\n",
       "      <td>113343</td>\n",
       "      <td>[CI] DocsClientYamlTestSuiteIT test {yaml=refe...</td>\n",
       "      <td>2024-09-22T04:44:47Z</td>\n",
       "      <td>2024-11-05T16:04:07Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>**Build Scans:**\\n- [elasticsearch-periodic-pl...</td>\n",
       "      <td>[-0.0024424961302429438, -0.006016217637807131...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _type          _repo _issueNumber  \\\n",
       "3906  issue  elasticsearch       113347   \n",
       "3907  issue  elasticsearch       113346   \n",
       "3908  issue  elasticsearch       113345   \n",
       "3909  issue  elasticsearch       113344   \n",
       "3910  issue  elasticsearch       113343   \n",
       "\n",
       "                                                 _title            _createdAt  \\\n",
       "3906              Update JDK version in CONTRIBUTING.md  2024-09-22T16:38:04Z   \n",
       "3907                 deps(updatecli): bump all policies  2024-09-22T06:22:37Z   \n",
       "3908  [CI] KibanaUserRoleIntegTests testSearchAndMSe...  2024-09-22T05:36:19Z   \n",
       "3909  [CI] RollupIndexerStateTests testMultipleJobTr...  2024-09-22T05:16:59Z   \n",
       "3910  [CI] DocsClientYamlTestSuiteIT test {yaml=refe...  2024-09-22T04:44:47Z   \n",
       "\n",
       "                 _closedAt  _state  \\\n",
       "3906  2024-09-26T21:20:39Z  closed   \n",
       "3907  2024-09-23T06:22:34Z  closed   \n",
       "3908  2024-09-25T10:16:00Z  closed   \n",
       "3909  2024-11-13T17:28:03Z  closed   \n",
       "3910  2024-11-05T16:04:07Z  closed   \n",
       "\n",
       "                                                  _body  \\\n",
       "3906  This week, I set up a local development enviro...   \n",
       "3907  \\n\\n\\n<Actions>\\n    <action id=\"90caa11f1dfdd...   \n",
       "3908  **Build Scans:**\\n- [elasticsearch-periodic #4...   \n",
       "3909  **Build Scans:**\\n- [elasticsearch-periodic #4...   \n",
       "3910  **Build Scans:**\\n- [elasticsearch-periodic-pl...   \n",
       "\n",
       "                                    GitHub_Issue_vector  \n",
       "3906  [0.006106697954237461, 0.0008585970499552786, ...  \n",
       "3907  [-0.011111822910606861, -0.005381433758884668,...  \n",
       "3908  [0.004877804312855005, -0.006739743519574404, ...  \n",
       "3909  [-0.027301553636789322, -0.016045166179537773,...  \n",
       "3910  [-0.0024424961302429438, -0.006016217637807131...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the new Column is created\n",
    "df_Issues.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682465d1-0413-4e65-8f0d-62a1579bb3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95e897aa-f073-4e35-9c2f-645ae5fdee44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Elasticsearch connection\n",
    "from elasticsearch import Elasticsearch,helpers\n",
    "es = Elasticsearch(['http://localhost:9200'])\n",
    "es.ping()   #connection testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8622726-a3cf-4a62-a0d9-8d1d57511de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118a2e8-5c50-49b0-aec5-882a76f606e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'github_issues'})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Index Mapping for githubissues\n",
    "\n",
    "index_mapping= {\n",
    "    \"properties\": {\n",
    "      \"GitHub_Issue_vector\": {\n",
    "          \"type\": \"dense_vector\",\n",
    "          \"dims\": 1536,\n",
    "          \"index\": \"true\",\n",
    "          \"similarity\": \"cosine\"\n",
    "      },\n",
    "     \"_type\": {\"type\": \"text\"}, \n",
    "     \"_repo\":{\"type\":\"text\"},   \n",
    "     \"_issueNumber\": {\"type\": \"long\"},    \n",
    "     \"_title\": {\"type\": \"text\"},\n",
    "     \"_createdAt\": {\"type\": \"date\"},\n",
    "     \"_closedAt\": {\"type\": \"date\"},\n",
    "     \"_state\": {\"type\": \"text\"},\n",
    "     \"_body\": {\"type\": \"text\"}\n",
    "   }\n",
    "}\n",
    "\n",
    "if es.indices.exists(index=\"github_issues\"):\n",
    "    es.indices.delete(index=\"github_issues\")\n",
    "\n",
    "es.indices.create(index=\"github_issues\", body={\"mappings\": index_mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dca088-74bc-4d7f-914f-a1d6d382a86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9aaa748c-0660-46ae-a0cc-9571ba1c5b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 3911 records into Elasticsearch. Failed records: []\n"
     ]
    }
   ],
   "source": [
    "# Bulk indexing for githubissues\n",
    "\n",
    "def dataframe_to_bulk_actions(df_Issues):\n",
    "    for index, row in df_Issues.iterrows():\n",
    "        yield {\n",
    "            \"_index\": 'github_issues',\n",
    "            \"_source\": {\n",
    "                \"_type\": row['_type'],\n",
    "                \"_repo\":row['_repo'],\n",
    "                \"_issueNumber\": row['_issueNumber'],\n",
    "                \"_title\": row['_title'],\n",
    "                \"_createdAt\": row['_createdAt'],\n",
    "                \"_closedAt\": row['_closedAt'],\n",
    "                \"_state\": row['_state'],\n",
    "                \"_body\": row['_body'],\n",
    "                \"GitHub_Issue_vector\": row['GitHub_Issue_vector']\n",
    "            }\n",
    "        }\n",
    "\n",
    "start = 0\n",
    "end = len(df_Issues)\n",
    "batch_size = 500\n",
    "\n",
    "for batch_start in range(start, end, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, end)\n",
    "    batch_dataframe = df_Issues.iloc[batch_start:batch_end]\n",
    "    actions = list(dataframe_to_bulk_actions(df_Issues.iloc[start:end]))\n",
    "    \n",
    "success, failed = helpers.bulk(es, actions)\n",
    "print(f\"Inserted {success} records into Elasticsearch. Failed records: {failed}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac75e1-947c-40c7-bc53-d6159b9becaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
